@book{Bond_Fox_2007,
  title = {Applying the {{Rasch Model}}: {{Fundamental Measurement}} in the {{Human Sciences}}, {{Second Edition}}},
  shorttitle = {Applying the {{Rasch Model}}},
  author = {Bond, Trevor G. and Fox, Christine M.},
  date = {2007-04-01},
  edition = {2},
  publisher = {Psychology Press},
  location = {New York},
  doi = {10.4324/9781410614575},
  abstract = {Written in an accessible style, this book facilitates a deep understanding of the Rasch model. Authors Bond and Fox review the crucial properties of the Rasch model and demonstrate its use with a wide range of examples including the measurement of educational achievement, human development, attitudes, and medical rehabilitation. A glossary and numerous illustrations further aid the reader's understanding. The authors demonstrate how to apply Rasch analysis and prepare readers to perform their own analyses and interpret the results. Updated throughout, highlights of the Second Edition include: a new CD that features an introductory version of the latest Winsteps program and the data files for the book’s examples, preprogrammed to run using Winsteps; a new chapter on invariance that highlights the parallels between physical and human science measurement; a new appendix on analyzing data to help those new to Rasch analysis; more explanation of the key concepts and item characteristic curves; a new empirical example with data sets demonstrates the many facets of the~Rasch model and other new examples; and  an increased focus on issues related to unidimensionality, multidimensionality, and the Rasch factor analysis of residuals. Applying the Rasch Model is intended for researchers and practitioners in psychology, especially developmental psychologists, education, health care, medical rehabilitation, business, government, and those interested in measuring attitude, ability, and/or performance. The book is an excellent text for use in courses on advanced research methods, measurement, or quantitative analysis. Significant knowledge of statistics is not required.},
  isbn = {978-1-4106-1457-5},
  pagetotal = {360},
  keywords = {/unread}
}

@article{Drasgow_1989,
  title = {An {{Evaluation}} of {{Marginal Maximum Likelihood Estimation}} for the {{Two-Parameter Logistic Model}}},
  author = {Drasgow, Fritz},
  date = {1989-03-01},
  journaltitle = {Applied Psychological Measurement},
  volume = {13},
  number = {1},
  pages = {77--90},
  publisher = {SAGE Publications Inc},
  issn = {0146-6216},
  doi = {10.1177/014662168901300108},
  url = {https://doi.org/10.1177/014662168901300108},
  urldate = {2024-03-21},
  abstract = {The accuracy of marginal maximum likelihood esti mates of the item parameters of the two-parameter lo gistic model was investigated. Estimates were obtained for four sample sizes and four test lengths; joint maxi mum likelihood estimates were also computed for the two longer test lengths. Each condition was replicated 10 times, which allowed evaluation of the accuracy of estimated item characteristic curves, item parameter estimates, and estimated standard errors of item pa rameter estimates for individual items. Items that are typical of a widely used job satisfaction scale and moderately easy tests had satisfactory marginal esti mates for all sample sizes and test lengths. Larger samples were required for items with extreme diffi culty or discrimination parameters. Marginal estima tion was substantially better than joint maximum like lihood estimation. Index terms: Fletcher-Powell algorithm, item parameter estimation, item response theory, joint maximum likelihood estimation, marginal maximum likelihood estimation, two-parameter logistic model.},
  langid = {english},
  file = {C:\Users\fabio\Zotero\storage\SJZWM8MA\Drasgow - 1989 - An Evaluation of Marginal Maximum Likelihood Estim.pdf}
}

@article{Finch_French_2019,
  title = {A Comparison of Estimation Techniques for {{IRT}} Models with Small Samples},
  author = {Finch, Holmes and French, Brian F.},
  date = {2019-04-03},
  journaltitle = {Applied Measurement in Education},
  volume = {32},
  number = {2},
  pages = {77--96},
  publisher = {Routledge},
  issn = {0895-7347},
  doi = {10.1080/08957347.2019.1577243},
  url = {https://doi.org/10.1080/08957347.2019.1577243},
  urldate = {2023-11-25},
  abstract = {The usefulness of item response theory (IRT) models depends, in large part, on the accuracy of item and person parameter estimates. For the standard 3 parameter logistic model, for example, these parameters include the item parameters of difficulty, discrimination, and pseudo-chance, as well as the person ability parameter. Several factors impact traditional marginal maximum likelihood (ML) estimation of IRT model parameters, including sample size, with smaller samples generally being associated with lower parameter estimation accuracy, and inflated standard errors for the estimates. Given this deleterious impact of small samples on IRT model performance, use of these techniques with low-incidence populations, where it might prove to be particularly useful, estimation becomes difficult, especially with more complex models. Recently, a Pairwise estimation method for Rasch model parameters has been suggested for use with missing data, and may also hold promise for parameter estimation with small samples. This simulation study compared item difficulty parameter estimation accuracy of ML with the Pairwise approach to ascertain the benefits of this latter method. The results support the use of the Pairwise method with small samples, particularly for obtaining item location estimates.},
  keywords = {simulation example},
  file = {C:\Users\fabio\Zotero\storage\8FB2V6LH\Finch and French - 2019 - A Comparison of Estimation Techniques for IRT Mode.pdf}
}

@article{Goncalves_etal_2023,
  title = {Flexible {{Bayesian}} Modelling in Dichotomous Item Response Theory Using Mixtures of Skewed Item Curves},
  author = {Gonçalves, Flávio B. and Venturelli S. L., Juliane and Loschi, Rosangela H.},
  date = {2023},
  journaltitle = {British Journal of Mathematical and Statistical Psychology},
  volume = {76},
  number = {1},
  pages = {69--86},
  issn = {2044-8317},
  doi = {10.1111/bmsp.12282},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/bmsp.12282},
  urldate = {2024-05-07},
  abstract = {Most item response theory (IRT) models for dichotomous responses are based on probit or logit link functions which assume a symmetric relationship between the probability of a correct response and the latent traits of individuals taking a test. This assumption restricts the use of those models to the case in which all items behave symmetrically. On the other hand, asymmetric models proposed in the literature impose that all the items in a test behave asymmetrically. This assumption is inappropriate for great majority of tests which are, in general, composed of both symmetric and asymmetric items. Furthermore, a straightforward extension of the existing models in the literature would require a prior selection of the items' symmetry/asymmetry status. This paper proposes a Bayesian IRT model that accounts for symmetric and asymmetric items in a flexible but parsimonious way. That is achieved by assigning a finite mixture prior to the skewness parameter, with one of the mixture components being a point mass at zero. This allows for analyses under both model selection and model averaging approaches. Asymmetric item curves are designed through the centred skew normal distribution, which has a particularly appealing parametrization in terms of parameter interpretation and computational efficiency. An efficient Markov chain Monte Carlo algorithm is proposed to perform Bayesian inference and its performance is investigated in some simulated examples. Finally, the proposed methodology is applied to a data set from a large-scale educational exam in Brazil.},
  langid = {english},
  keywords = {Markov chain Monte Carlo,point-mass mixture priors,skew normal distribution},
  file = {C:\Users\fabio\Zotero\storage\FLC3K7YZ\Gonçalves et al. - 2023 - Flexible Bayesian modelling in dichotomous item re.pdf}
}

@article{Konig_etal_2020,
  title = {An {{Optimized Bayesian Hierarchical Two-Parameter Logistic Model}} for {{Small-Sample Item Calibration}}},
  author = {König, Christoph and Spoden, Christian and Frey, Andreas},
  date = {2020-06},
  journaltitle = {Applied Psychological Measurement},
  shortjournal = {Appl Psychol Meas},
  volume = {44},
  number = {4},
  eprint = {32536732},
  eprinttype = {pmid},
  pages = {311--326},
  issn = {0146-6216},
  doi = {10.1177/0146621619893786},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7262992/},
  urldate = {2024-03-04},
  abstract = {Accurate item calibration in models of item response theory (IRT) requires rather large samples. For instance, N{$>$}500 respondents are typically recommended for the two-parameter logistic (2PL) model. Hence, this model is considered a large-scale application, and its use in small-sample contexts is limited. Hierarchical Bayesian approaches are frequently proposed to reduce the sample size requirements of the 2PL. This study compared the small-sample performance of an optimized Bayesian hierarchical 2PL (H2PL) model to its standard inverse Wishart specification, its nonhierarchical counterpart, and both unweighted and weighted least squares estimators (ULSMV and WLSMV) in terms of sampling efficiency and accuracy of estimation of the item parameters and their variance components. To alleviate shortcomings of hierarchical models, the optimized H2PL (a) was reparametrized to simplify the sampling process, (b) a strategy was used to separate item parameter covariances and their variance components, and (c) the variance components were given Cauchy and exponential hyperprior distributions. Results show that when combining these elements in the optimized H2PL, accurate item parameter estimates and trait scores are obtained even in sample sizes as small as N=100. This indicates that the 2PL can also be applied to smaller sample sizes encountered in practice. The results of this study are discussed in the context of a recently proposed multiple imputation method to account for item calibration error in trait estimation.},
  pmcid = {PMC7262992},
  file = {C:\Users\fabio\Zotero\storage\33PU9ZIK\König et al. - 2020 - An Optimized Bayesian Hierarchical Two-Parameter L.pdf}
}

@article{Lee_Bolt_2018a,
  title = {Asymmetric Item Characteristic Curves and Item Complexity: Insights from Simulation and Real Data Analyses},
  shorttitle = {Asymmetric Item Characteristic Curves and Item Complexity},
  author = {Lee, Sora and Bolt, Daniel M.},
  date = {2018-06},
  journaltitle = {Psychometrika},
  shortjournal = {Psychometrika},
  volume = {83},
  number = {2},
  eprint = {28948426},
  eprinttype = {pmid},
  pages = {453--475},
  issn = {1860-0980},
  doi = {10.1007/s11336-017-9586-5},
  abstract = {While item complexity is often considered as an item feature in test development, it is much less frequently attended to in the psychometric modeling of test items. Prior work suggests that item complexity may manifest through asymmetry in item characteristics curves (ICCs; Samejima in Psychometrika 65:319-335, 2000). In the current paper, we study the potential for asymmetric IRT models to inform empirically about underlying item complexity, and thus the potential value of asymmetric models as tools for item validation. Both simulation and real data studies are presented. Some psychometric consequences of ignoring asymmetry, as well as potential strategies for more effective estimation of asymmetry, are considered in discussion.},
  langid = {english},
  keywords = {Analysis of Variance,asymmetric item characteristic curves,Computer Simulation,Educational Measurement,Humans,item complexity,item response theory,item validation,Mathematical Concepts,Models Psychological,Psychometrics,Reaction Time,Regression Analysis},
  file = {C:\Users\fabio\Zotero\storage\48ZB5RXU\Lee and Bolt - 2018 - Asymmetric Item Characteristic Curves and Item Com.pdf}
}

@article{Liu_Yang_2018,
  title = {Interval {{Estimation}} of {{Latent Variable Scores}} in {{Item Response Theory}}},
  author = {Liu, Yang and Yang, Ji Seung},
  date = {2018-06-01},
  journaltitle = {Journal of Educational and Behavioral Statistics},
  volume = {43},
  number = {3},
  pages = {259--285},
  publisher = {American Educational Research Association},
  issn = {1076-9986},
  doi = {10.3102/1076998617732764},
  url = {https://doi.org/10.3102/1076998617732764},
  urldate = {2024-03-04},
  abstract = {The uncertainty arising from item parameter estimation is often not negligible and must be accounted for when calculating latent variable (LV) scores in item response theory (IRT). It is particularly so when the calibration sample size is limited and/or the calibration IRT model is complex. In the current work, we treat two-stage IRT scoring as a predictive inference problem: The target of prediction is a random variable that follows the true posterior of the LV conditional on the response pattern being scored. Various Bayesian, fiducial, and frequentist prediction intervals of LV scores, which can be obtained from a simple yet generic Monte Carlo recipe, are evaluated and contrasted via simulations based on several measures of prediction quality. An empirical data example is also presented to illustrate the use of candidate methods.},
  langid = {english},
  file = {C:\Users\fabio\Zotero\storage\I3AL2VSA\Liu and Yang - 2018 - Interval Estimation of Latent Variable Scores in I.pdf}
}

@software{Revelle_2024,
  title = {{{psychTools}}: {{Tools}} to {{Accompany}} the 'psych' {{Package}} for {{Psychological Research}}},
  shorttitle = {{{psychTools}}},
  author = {Revelle, William},
  date = {2024-03-19},
  url = {https://cran.r-project.org/web/packages/psychTools/index.html},
  urldate = {2024-07-02},
  abstract = {Support functions, data sets, and vignettes for the 'psych' package. Contains several of the biggest data sets for the 'psych' package as well as four vignettes. A few helper functions for file manipulation are included as well. For more information, see the {$<$}https://personality-project.org/r/{$>$} web page.},
  version = {2.4.3},
  keywords = {/unread,Psychometrics}
}

@article{Shim_etal_2023,
  title = {Parsimonious Item Response Theory Modeling with the Negative Log-Log Link: The Role of Inflection Point Shift},
  shorttitle = {Parsimonious Item Response Theory Modeling with the Negative Log-Log Link},
  author = {Shim, Hyejin and Bonifay, Wes and Wiedermann, Wolfgang},
  date = {2023-08-03},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02189-z},
  url = {https://doi.org/10.3758/s13428-023-02189-z},
  urldate = {2023-11-09},
  abstract = {In item response theory (IRT) modeling, the magnitude of the lower and upper asymptote parameters determines the degree to which the inflection point shifts above or below P = 0.50. The current study examines the one-parameter negative log-log model (NLLM), which is characterized by a downward shift in the inflection point, among other distinctive psychometric properties. After detailing the statistical foundations of the NLLM, we present a series of simulation studies to establish item and person parameter estimation accuracy and to demonstrate that this parsimonious model addresses the “slipping” effect (i.e., unexpectedly incorrect answers) via an inflection point {$<$} 0.50 rather than through computationally difficult estimation of the upper asymptote. We then provide further support for these simulation results through empirical data analysis. Finally, we discuss how the NLLM contributes to recent methodological literature on the utility of asymmetric IRT models.},
  langid = {english},
  keywords = {Asymmetric model,Generalized linear models,Inflection point shift,Item response theory,Measurement,Model complexity,Psychometrics,Upper asymptote parameter},
  file = {C:\Users\fabio\Zotero\storage\8BEAXMXE\Shim et al. - 2023 - Parsimonious item response theory modeling with th.pdf}
}

@article{Shim_etal_2023a,
  title = {Parsimonious Asymmetric Item Response Theory Modeling with the Complementary Log-Log Link},
  author = {Shim, Hyejin and Bonifay, Wes and Wiedermann, Wolfgang},
  date = {2023-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res Methods},
  volume = {55},
  number = {1},
  eprint = {35355241},
  eprinttype = {pmid},
  pages = {200--219},
  issn = {1554-3528},
  doi = {10.3758/s13428-022-01824-5},
  abstract = {Traditional item response theory (IRT) models assume a symmetric error distribution and rely on symmetric (logit or probit) link functions to model the response probabilities. As an alternative, we investigated the one-parameter complementary log-log model (CLLM), which is founded on an asymmetric error distribution and results in an asymmetric item response function with important psychometric properties. In a series of simulation studies, we demonstrate that the CLLM (a) is estimable in small sample sizes, (b) facilitates item-weighted scoring, and (c) accounts for the effect of guessing, despite the presence of a single parameter. We then provide further evidence for these claims by applying the CLLM to empirical data. Finally, we discuss how this work contributes to the growing psychometric literature on model complexity.},
  langid = {english},
  keywords = {Complementary Log-Log,Computer Simulation,Generalized linear models,Humans,Item response theory,Measurement,Model complexity,Probability,Psychometrics,Sample Size},
  file = {C:\Users\fabio\Zotero\storage\ZTTVGLUZ\Shim et al. - 2023 - Parsimonious asymmetric item response theory model.pdf}
}

@article{Vehtari_etal_2017,
  title = {Practical Bayesian Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  date = {2017-09-01},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {27},
  number = {5},
  pages = {1413--1432},
  issn = {1573-1375},
  doi = {10.1007/s11222-016-9696-4},
  url = {https://doi.org/10.1007/s11222-016-9696-4},
  urldate = {2024-02-07},
  abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
  langid = {english},
  keywords = {Bayesian computation,K-fold cross-validation,Leave-one-out cross-validation (LOO),Pareto smoothed importance sampling (PSIS),Stan,Widely applicable information criterion (WAIC)},
  file = {C:\Users\fabio\Zotero\storage\AJ6LQFB3\Vehtari et al. - 2017 - Practical bayesian model evaluation using leave-on.pdf}
}

@article{Yao_etal_2018,
  title = {Using Stacking to Average Bayesian Predictive Distributions (with Discussion)},
  author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
  date = {2018-09},
  journaltitle = {Bayesian Analysis},
  volume = {13},
  number = {3},
  pages = {917--1007},
  publisher = {International Society for Bayesian Analysis},
  issn = {1936-0975, 1931-6690},
  doi = {10.1214/17-BA1091},
  url = {https://projecteuclid.org/journals/bayesian-analysis/volume-13/issue-3/Using-Stacking-to-Average-Bayesian-Predictive-Distributions-with-Discussion/10.1214/17-BA1091.full},
  urldate = {2023-10-15},
  abstract = {Bayesian model averaging is flawed in the M-open setting in which the true data-generating process is not one of the candidate models being fit. We take the idea of stacking from the point estimation literature and generalize to the combination of predictive distributions. We extend the utility function to any proper scoring rule and use Pareto smoothed importance sampling to efficiently compute the required leave-one-out posterior distributions. We compare stacking of predictive distributions to several alternatives: stacking of means, Bayesian model averaging (BMA), Pseudo-BMA, and a variant of Pseudo-BMA that is stabilized using the Bayesian bootstrap. Based on simulations and real-data applications, we recommend stacking of predictive distributions, with bootstrapped-Pseudo-BMA as an approximate alternative when computation cost is an issue.},
  keywords = {Bayesian model averaging,model combination,predictive distribution,proper scoring rule,stacking,Stan},
  file = {C:\Users\fabio\Zotero\storage\WJ8X827V\Yao et al. - 2018 - Using Stacking to Average Bayesian Predictive Dist.pdf}
}

@article{Zhang_etal_2022,
  title = {Bayesian Item Response Theory Models with Flexible Generalized Logit Links},
  author = {Zhang, Jiwei and Zhang, Ying-Ying and Tao, Jian and Chen, Ming-Hui},
  date = {2022-07-01},
  journaltitle = {Applied Psychological Measurement},
  volume = {46},
  number = {5},
  pages = {382--405},
  publisher = {SAGE Publications Inc},
  issn = {0146-6216},
  doi = {10.1177/01466216221089343},
  url = {https://doi.org/10.1177/01466216221089343},
  urldate = {2023-11-20},
  abstract = {In educational and psychological research, the logit and probit links are often used to fit the binary item response data. The appropriateness and importance of the choice of links within the item response theory (IRT) framework has not been investigated yet. In this paper, we present a family of IRT models with generalized logit links, which include the traditional logistic and normal ogive models as special cases. This family of models are flexible enough not only to adjust the item characteristic curve tail probability by two shape parameters but also to allow us to fit the same link or different links to different items within the IRT model framework. In addition, the proposed models are implemented in the Stan software to sample from the posterior distributions. Using readily available Stan outputs, the four Bayesian model selection criteria are computed for guiding the choice of the links within the IRT model framework. Extensive simulation studies are conducted to examine the empirical performance of the proposed models and the model fittings in terms of “in-sample” and “out-of-sample” predictions based on the deviance. Finally, a detailed analysis of the real reading assessment data is carried out to illustrate the proposed methodology.},
  langid = {english},
  file = {C:\Users\fabio\Zotero\storage\7GBM8X6K\Zhang et al. - 2022 - Bayesian Item Response Theory Models With Flexible.pdf}
}
