---
title: "Bayesian Model Averaging of (a)symmetric IRT Models in Small Samples"
title-slide-attributes:
  
   data-background-image: images/
   data-background-size: 83%
   data-background-opacity: 100%

author: "Fabio Setti & Leah Feuerstahler"
format:
   revealjs:
      footer: "IMPS 2024"
      chalkboard: true 
      theme: Theme/IMPS_2024_theme.scss
      navigation-mode: linear
      controls: false
      slide-number: c 
      width: 1280
      height: 720
      transition-speed: slow
      css: Style.css
      
  
editor: source
---


## Item Response Theory

**Item response theory (IRT)** is a branch of psychometrics that models how both **person** and **item** characteristics influence the probability of discrete responses to items.   

::: {.fragment .fade-in fragment-index="1"}
- **Item parameters:** characteristics of an item itself ($i$).

- **Person parameters:** Some latent trait ($\theta$) varying across individuals ($j$).
:::


::: {layout-ncol="2"}

:::col

</br>

</br>

::: {.fragment .fade-in fragment-index="2"}
For example, the 2 parameter logistic model (2PL): 


$$P(Y^j_{i} = 1| \theta_j, a_i, b_i) =  \frac{\exp[a_i(\theta_j - b_i)]}{1 + \exp[a_i(\theta_j - b_i)]}$$
</br>
:::

::: {.fragment .fade-in fragment-index="3"}
<center>
$a$ = discrimination

$b$ = difficulty

$\theta$ = latent trait

</center>
:::
:::

:::col

</br>
</br>

::: {.fragment .fade-in fragment-index="4"}
![](images/two_pl.png)
:::
:::
:::


## Adding parameters

::: {.panel-tabset}

## 1PL


::: {layout-ncol="2"}

:::col


</br>
</br>
</br>
</br>
</br>

<center>

$$P(Y = 1| \theta, b) =  \frac{\exp[(\theta - b)]}{1 + \exp[(\theta - b)]}$$


$\theta$ = latent trait

$b$ = difficulty

</center>

:::



:::col

</br>
</br>
</br>


![](images/one_pl.png)

:::
:::

## 2PL


::: {layout-ncol="2"}

:::col


</br>
</br>
</br>
</br>
</br>

<center>

$$P(Y = 1| \theta, b, a) =  \frac{\exp[a(\theta - b)]}{1 + \exp[a(\theta - b)]}$$

$\theta$ = latent trait

$b$ = difficulty

$a$ =  discrimination

</center>

:::



:::col

</br>
</br>
</br>


![](images/two_pl.png)

:::
:::


## 3PL



::: {layout-ncol="2"}

:::col


</br>
</br>
</br>
</br>
</br>

<center>

$$P(Y = 1| \theta, b, a, c) =  c + (1-c) \frac{\exp[a(\theta - b)]}{1 + \exp[a(\theta - b)]}$$

$\theta$ = latent trait

$b$ = difficulty

$a$ =  discrimination

$c$ = guessing

</center>

:::



:::col

</br>
</br>
</br>


![](images/three_pl.png)

:::
:::




## 4PL

::: {layout-ncol="2"}

:::col


</br>
</br>
</br>
</br>


<center>

$$P(Y = 1| \theta, b, a, c, d) =  c + (d-c) \frac{\exp[a(\theta - b)]}{1 + \exp[a(\theta - b)]}$$

$\theta$ = latent trait

$b$ = difficulty

$a$ = discrimination

$c$ = guessing

$d$ = slipping

</center>

:::



:::col

</br>
</br>
</br>


![](images/four_pl.png)

:::
:::
:::

## Estimation and Sample Size 

The more item parameters are added to the model, the more flexible the item response functions (IRF).

::: {.fragment .fade-in fragment-index="1"}
However, the additional parameters of the **3PL** and **4PL** tend to require large sample sizes ($N \geq 1000$) to be stably estimated.
:::

</br>

::: {.fragment .fade-in fragment-index="2"}
<center>  What about **1PL** and **2PL** models? </center>
:::

</br>


::: {layout-ncol="2"}

:::col

::: {.fragment .fade-in fragment-index="2"}
<center> **Maximum Likelihood** </center>
:::

</br>

::: {.fragment .fade-in fragment-index="3"}

<i class="fa fa-solid fa-thumbs-up" style="color: #9B3922;"></i> 1PL models seem to be stably estimable with sample sizes as low as $N = 100$ (Finch & French, 2019).

</br>

<i class="fa fa-solid fa-thumbs-down" style="color: #9B3922;"></i> 2PL models seems to require a sample size of $N = 200$ or more (Drasgow, 1989; Liu & Yang, 2018).


:::

:::


:::col

::: {.fragment .fade-in fragment-index="2"}
<center> **Markov Chain Monte Carlo** </center>
:::

</br>

::: {.fragment .fade-in fragment-index="4"}
<i class="fa fa-solid fa-thumbs-up" style="color: #9B3922;"></i></i> 1PL models showed good coverage when $N = 100$ and generally outperformed maximum likelihood (Finch & French, 2019).


</br>

<i class="fa fa-solid fa-thumbs-up" style="color: #9B3922;"></i> 2PL models with hierarchical priors perform reasonably well even when $N = 100$ (k&ouml;nig et al., 2020).
:::


:::
:::

::: {.fragment .fade-in fragment-index="2"}
<style>
.vl {
  border-left: 3px solid #9B3922;
  height: 400px;
  position: absolute;
  left: 49%;
  margin-left: 10px;
  top: 43%;
}
</style>

<div class="vl"></div>
:::


## Asymmetric IRT Models

::: {layout-ncol="2"}

:::col

::: {.fragment .fade-in fragment-index="1"}
All the IRT models presented so far generate IRFs that are **symmetric**.

</br>
</br>

![](images/symmetric_irf.png)

:::
:::

:::col
::: {.fragment .fade-in fragment-index="2"}

Samejima (2000) was the first scholar to propose the use of **asymmetric** IRT models with her logistic positive exponent (LPE) model.


</br>

![](images/asymmetric_irf.png)
:::

:::
:::


## Simple Asymmetric IRT Models

The LPE has shown to have some identification issues in simulation studies (Lee & Bolt, 2018), especially in small sample sizes. Two recently proposed asymmetric IRT models (Shim et al., 2023) may help address this issue:

::: {layout-ncol="2"}

:::col

::: {.fragment .fade-in fragment-index="1"}
<center> 


**Complementary Log-Log (CLL)**

$$ P(Y = 1| \theta) = 1 - \exp[-\exp[a(\theta - b)]]$$


![](images/CLL_vs_3pl.png)

</center>
:::
:::




:::col
::: {.fragment .fade-in fragment-index="2"}
<center> 

**Negative Log-Log (NLL)**

$$ P(Y = 1| \theta) =  \exp[-\exp[-a(\theta - b)]]$$
![](images/CLL_vs_3plU.png)


</center>
:::

:::

:::


## What to do about Small Sample Sizes? 

Although the NLL and CLL may approximate more complex models, complex IRFs remain hard to approximate in small sample sizes ($N \leq 250$) with a single model.

</br>

::: {.fragment .fade-in fragment-index="1"}
<center> Can we do better with **Bayesian model averaging (BMA)**? </center>
:::

</br>

::: {.fragment .fade-in fragment-index="2"}
**Model averaging** takes into account model uncertainty by weighting model predictions according to model's relative plausibility.
:::

::: {.fragment .fade-in fragment-index="3"}
The core of **BMA** is the *expected log pointwise predictive density*, $ELPD = \sum log(p(y_i |y_{-i}, M_k))$, which is an approximation of leave-one-out cross validation ($LOO_{CV}$).
:::
</br>

::: {layout-ncol="2"}

:::col
::: {.fragment .fade-in fragment-index="4"}

**Two type of weights:**

- BMA weights with Bayesian bootstrapping (BMA+)

- Stacking weights

:::
:::

:::col


::: {.fragment .fade-in fragment-index="5"}
<center>
**Idea:** How much better can BMA of simple symmetric and asymmetric models do compared to model selection (MS) in the context of IRT? 
</center>
:::

:::
:::

## Averaging Predicted Probability: The Scale of $\theta$ 

::: {layout-ncol="2"}


:::col
::: {.fragment .fade-in fragment-index="1"}

<center> **Ideal Scenario** </center>

Ultimately, one would like to get the best possible estimate of $P(Y = 1|\theta)$ by averaging along the $\theta$ continuum.
![](images/avg_plot.png)

:::
:::

:::col
::: {.fragment .fade-in fragment-index="2"}

<center> **Reality** </center>

However, the same person will get a different $\theta$ depending on the model that is fit to the data. 

</br>
</br>

![](images/theta_plot.png)

:::
:::
:::


## Empirical and Theoretical Quantiles of $\theta$ 

::: {.fragment .fade-in fragment-index="1"}
Although the same person may be assigned a different $\theta$ depending on the model, the **relative rank** of participants should be scale invariant.
:::

::: {.fragment .fade-in fragment-index="2"}
This means that instead of averaging IRFs at the same $\theta$ values, it should be more sound to average IRFs at the same $\theta$ **quantile**. 
:::

::: {.fragment .fade-in fragment-index="3"}
These quantiles will be estimated empirically for each of the models to be averaged. Thus, the averaged probability of a keyed response at each **empirical quantile** will be 
:::

::: {.fragment .fade-in fragment-index="4"}
$$\overline{IRF}_{q} = \sum_{1}^{m} W_{m}P_{m}(\theta_{mq})$$
where $q$ represents a specific quantile, $m$ represents one of the candidate models, and $W$ represents the weight assigned to each model.
:::

</br>

::: {.fragment .fade-in fragment-index="5"}
For comparison, IRFs will also be averaged at **theoretical quantiles** of the standard normal distribution.
:::



## {}


</br>

::: {.fragment .fade-in fragment-index="1"}
- Is it possible to leverage simple symmetric and asymmetric IRT models (1PL, 2PL, 1CLL, 2CLL, 1NLL, 2NLL) to recover complex IRFs in **small samples**?
:::

</br>

::: {.fragment .fade-in fragment-index="2"}
- Which method among **BMA**, model selection (**MS**), and kernel smoothing IRT (**KS**) will produce better IRF 
recovery?
:::

</br>

::: {.fragment .fade-in fragment-index="3"}
- Will averaging at **empirical $\theta$ quantiles** achieve better IRF recovery than averaging at **theoretical $\theta$ quantiles**?
:::

</br>

::: {.fragment .fade-in fragment-index="4"}
- How will **stacking weights** and **BMA+ weights** behave? 
:::


## Simulation: Data Generation

Most data generating conditions were designed to be "realistic", where the true data generating model is not included in the set of candidate models. There were **4 data generating conditions**:

::: {layout-ncol="2"}




:::col

<center>
</br>
</br>

::: {.fragment .fade-in fragment-index="1"}
 **2PL:** $\frac{\exp[a(\theta - b)]}{1 + \exp[a(\theta - b)]}$
:::

</br>
</br>

::: {.fragment .fade-in fragment-index="2"}
 **2MPL:** $\frac{1}{1 +\exp[-(a_{1}\theta_{1} + a_{2}\theta_{2} + d)]}$
:::
</br>
</br>

::: {.fragment .fade-in fragment-index="3"}
 **GLL~ua~** and **GLL~la~** (Zhang et al., 2023)
:::
</center>

:::
:::col
::: r-stack


::: {.fragment .fade-in-then-out fragment-index="3"}
</br>

![](images/Stukel_fig.png)
:::

::: {.fragment .fade-in fragment-index="4"}


<center> **Parameters Generating Distributions** </center>

![](images/Tab_datagen.png)

:::

:::

:::
:::

::: {.fragment .fade-in fragment-index="5"}
<center> Each of these conditions will be crossed with **item = 10 & 20** and **sample size = 100 & 250** </center>
:::



## Estimation Method and IRF Averaging

The 6 IRT models (1PL, 2PL, 1CLL, 2CLL, 1NLL, 2NLL) will be estimated through MCMC using `brms` (B&uuml;rkner, 2017). How is IRF averaging implemented?

</br>

<center>

::: {.fragment .fade-in fragment-index="1"}
**1. Posterior distributions of parameters:** The MCMC sampler provides $N$ (6000 in this simulation) draws for each model parameter.
:::
 
::: {.fragment .fade-in fragment-index="2"} 
 &darr;
 
 **2. Posterior distributions of $P(Y = 1|\theta_q)$:** The predicted probability of a keyed response is calculated at a set of empirical or theoretical quantiles , $P(Y = 1|\theta_q)$, for all models across all MCMC draws. 
:::

::: {.fragment .fade-in fragment-index="3"}
 &darr;
 
**3. Model weights:** A weight (stacking or BMA+) is calculated for each model.
:::
  
  
::: {.fragment .fade-in fragment-index="4"}  
 &darr;
 

 **4. Averaged distribution of $P(Y = 1|\theta_q)$:** sample from each model distribution estimated in step 2 **in proportion to model weight**.
:::

</center>


## Priors and Outcome


::: {layout-ncol="2"}

:::col

<center> **Priors** </center>

Model estimation used the partial pooling approach (i.e., random intercepts/slopes)

</br>
</br>

<center>

::: {.fragment .fade-in fragment-index="1"}  

$log(\bar{a}) = N(0, 1)$

$\sigma_{log(\bar{a})} = N(0, 1)$

$\bar{b} = N(0, 3)$

$\sigma_{\bar{b}} = N(0, 3)$

$\bar{\theta} = 0$

$\sigma_\bar{\theta} = 1$

:::

</center>

:::


:::col

::: {.fragment .fade-in fragment-index="2"}  

<center> **Outcome** </center>

The only outcome will be the root mean squared error (**RMSE**) between the data generating IRF and estimated IRF:

:::

</br>
</br>

::: {.fragment .fade-in fragment-index="3"}  

$$RMSE_{iq} = \sqrt{\frac{1}{n_i}\sum_1^n(P(y = 1|\theta_{iq}) - P(y = 1|\hat{\theta}_{iqn}))^{2}}$$

$i =$ item.

$q =$ quantile (either empirical or theoretical) of $\theta$ distribution. 

$n =$ number of MCMC draws (6000). 

:::

:::
:::


## Results: Weight Distribution With 10 Items


<center>
::: {.fragment .fade-in fragment-index="1"} 

![](images/Weights_10_items.png){fig-align="center" width=65%}
:::

</center>


## Results: Weight Distribution With 20 Items


<center>

::: {.fragment .fade-in fragment-index="1"} 
![](images/Weights_20_items.png){fig-align="center" width=65%}
:::

</center>


## RMSE for the 2PL & 2MPL


::: {layout-ncol="2"}

:::col
::: {.fragment .fade-in fragment-index="1"}  

<center> **2PL** </center>

![](images/rmse_2PL.png){fig-align="center" width=85%}
:::
:::

:::col
::: {.fragment .fade-in fragment-index="2"}  
<center> **2MPL** </center>

![](images/rmse_2MPL.png){fig-align="center" width=85%}
:::
:::

:::


## RMSE for the GLL~la~ & GLL~ua~


::: {layout-ncol="2"}

:::col
::: {.fragment .fade-in fragment-index="1"}  

<center> **GLL~la~** </center>

![](images/rmse_GLL_la.png){fig-align="center" width=85%}

:::
:::

:::col
::: {.fragment .fade-in fragment-index="2"}  

<center> **GLL~ua~** </center>

![](images/rmse_GLL_ua.png){fig-align="center" width=85%}

:::
:::

:::

## Summary of Results

::: {.fragment .fade-in fragment-index="1"}  
- BMA offers better IRF recovery compared to both MS and KS.
:::

</br>

::: {.fragment .fade-in fragment-index="2"}  
- BMA's advantage over MS is more noticeable at low/high quantiles, with smaller sample sizes, and with less items.  
:::

</br>

::: {.fragment .fade-in fragment-index="3"}  
- Stacking weights and BMA+ weights behave differently but offer comparable IRF recovery. 
:::

</br>

::: {.fragment .fade-in fragment-index="4"}  
- Empirical quantiles seemed to offer slight improvements over theoretical quantiles.
:::

</br>

:::{.fragment .fade-in fragment-index="5"}  
- The 2PL gained weight as sample size and number of items increased.
:::

## Future Directions: Averaging By Item?

::: {.fragment .fade-in fragment-index="1"}  
<p style="font-size: 24px;"> As mentioned in the introduction, the $LOO_{CV}$ is a **pointwise** measures of fit. So, every single data point has a "fit measure" associated with it! </p>
:::

::: {.fragment .fade-in fragment-index="2"}  
<p style="font-size: 24px;"> Models are compared by summing these pointwise fit measures across all data points. [What if we compared these fit measures also across ***items*** rather than just across models?]{.fragment .fade-in fragment-index="3"}  </p>
:::


::: {layout-ncol="2"}

:::col

::: {.fragment .fade-in fragment-index="4"}

![](images/Rotate_plot.png){width=95%}
:::
:::

:::col
::: {.fragment .fade-in fragment-index="5"}  

![](images/Letters_plot.png){width=95%}
:::
:::
:::

::: {.fragment .fade-in fragment-index="6"}  
<center> Globally, the 2PL fits best, but that is not necessarily the case for all the items on the test. </center>
:::



# Fin 🤷












