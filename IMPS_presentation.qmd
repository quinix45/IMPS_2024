---
title: "Bayesian Model Averaging of (a)symmetric IRT Models in Small Samples"
title-slide-attributes:
  
   data-background-image: images/
   data-background-size: 83%
   data-background-opacity: 100%

author: "Fabio Setti & Leah Feuerstahler"
institute: "Fordham University"
bibliography: IMPS presetnation bibliography.bib
csl: apa.csl
format:
   revealjs:
      footer: "IMPS 2024"
      chalkboard: true 
      theme: Theme/IMPS_2024_theme.scss
      navigation-mode: linear
      controls: false
      slide-number: c 
      width: 1280
      height: 720
      transition-speed: slow
      css: Style.css
      
  
editor: source
---

## Estimation and Sample Size in IRT

The more item parameters are added to the model, the more flexible the item response functions (IRFs). 

::: {.fragment .fade-in fragment-index="1"}
However, the additional parameters of the **3PL** and **4PL** tend to require large sample sizes ($N \geq 1000$) to be stably estimated.
:::

</br>

::: {.fragment .fade-in fragment-index="2"}
<center>  We will be focusing on **1PL** and **2PL** models </center>
:::

</br>


::: {layout-ncol="2"}

:::col

::: {.fragment .fade-in fragment-index="3"}
<center> **Maximum Likelihood** </center>
:::

</br>

::: {.fragment .fade-in fragment-index="3"}

<i class="fa fa-solid fa-thumbs-up" style="color: #9B3922;"></i> 1PL models seem to be stably estimable with sample sizes as low as $N = 100$ [@Finch_French_2019].

</br>

<i class="fa fa-solid fa-thumbs-down" style="color: #9B3922;"></i> 2PL models seems to require a sample size of $N = 200$ or more [@Drasgow_1989; @Liu_Yang_2018].


:::

:::


:::col

::: {.fragment .fade-in fragment-index="3"}
<center> **Markov Chain Monte Carlo** </center>
:::

</br>

::: {.fragment .fade-in fragment-index="3" .absolute top=62 left=20}
<i class="fa fa-solid fa-thumbs-up" style="color: #9B3922;"></i></i> 1PL models showed good coverage when $N = 100$ and generally outperformed maximum likelihood [@Finch_French_2019].


</br>

<i class="fa fa-solid fa-thumbs-up" style="color: #9B3922;"></i> 2PL models with hierarchical priors perform reasonably well even when $N = 100$ [@Konig_etal_2020].
:::


:::
:::

::: {.fragment .fade-in fragment-index="3"}
<style>
.vl {
  border-left: 3px solid #9B3922;
  height: 400px;
  position: absolute;
  left: 49%;
  margin-left: 10px;
  top: 43%;
}
</style>

<div class="vl"></div>
:::

## Simple Asymmetric IRT Models

Most asymmetric models include asymmetry parameters that are hard to estimate in small sample sizes [e.g., @Goncalves_etal_2023; @Lee_Bolt_2018a; @Verkuilen_Johnson_2024a] Two recently proposed asymmetric IRT models [@Shim_etal_2023a; @Shim_etal_2023] may help address this issue:

::: {layout-ncol="2"}

:::col

::: {.fragment .fade-in fragment-index="1"}
<center> 


**Complementary Log-Log (CLL)**

$$ P(Y = 1| \theta) = 1 - \exp[-\exp[a(\theta - b)]]$$


![](images/CLL_vs_3pl.png)

</center>
:::
:::




:::col
::: {.fragment .fade-in fragment-index="1"}
<center> 

**Negative Log-Log (NLL)**

$$ P(Y = 1| \theta) =  \exp[-\exp[-a(\theta - b)]]$$
![](images/NLL_vs_3plU.png)


</center>
:::

:::

:::


## What to do about Small Sample Sizes? 

Although the NLL and CLL may approximate more complex models, complex IRFs remain hard to approximate in small sample sizes ($N \leq 250$) with a single model.

</br>

::: {.fragment .fade-in fragment-index="1"}
<center> Can we do better with **Bayesian model averaging (BMA)**? </center>
:::

</br>

::: {.fragment .fade-in fragment-index="2"}
**Model averaging** takes into account model uncertainty by **weighting** a set of candidate models according to their relative plausibility.
:::

::: {.fragment .fade-in fragment-index="3"}

**BMA** weights are based on a leave-one-out cross validation approximation [@Vehtari_etal_2017]. This provides a **fit measure for each data point**. 
:::
</br>

::: {layout-ncol="2"}

:::col


::: {.fragment .fade-in fragment-index="4"}
<center>

Calculating model weights:


- Test level weights 

::: {.fragment .fade-in fragment-index="5"}
- Item level weights? 
:::

</center>
:::

:::

:::col
::: {.fragment .fade-in fragment-index="6"}

**Two type of weights [@Yao_etal_2018]:**

- BMA weights with Bayesian bootstrapping (BMA+)

- Stacking weights

:::
:::


:::


## Averaging Predicted Probability of Keyed Responses

::: {layout-ncol="2"}


:::col
::: {.fragment .fade-in fragment-index="1"}

<center> **Theoretical quantiles** </center>

estimate $P(Y = 1|\theta)$ by averaging along a common $\theta$ continuum. Assumes a common $\theta$ scale across models.
 
![](images/avg_plot.png)

:::
:::

:::col
::: {.fragment .fade-in fragment-index="2"}

<center> **Empirical quantiles** </center>

estimate $P(Y = 1|\theta)$ by averaging along a common *empirical* $\theta$ continuum. Can accommodate different $\theta$ scales.

![](images/CLL_NLL.png){fig-align="center" width=90%}

:::
:::
:::


## Empirical Example 

We fit the 1PL, 2PL, 1CLL, 2CLL, 1NLL, 2NLL with the `brms` package [@Burkner_2017] to the Bond's Logical Operations Test [BLOT\; @Bond_Fox_2007] dataset from the `PsychTools` package [@Revelle_2024], which includes **150 participants** and **35 items**. 

::: {layout-ncol="2"}

:::col
::: {.fragment .fade-in fragment-index="1"}

![](images/empirical_example/plot_emp_weights.png)

:::
:::

:::col
::: {.fragment .fade-in fragment-index="2"}


<center> **Predictions for Item 14** </center>

![](images/empirical_example/Real Data Predictions.png)

:::
:::
:::

## Simulation 

Most data generating condition purposely introduced some type of model misspecification. There were **4 data generating conditions**:

::: {layout-ncol="2"}


:::col

<center>
</br>


::: {.fragment .fade-in fragment-index="1"}
 **2PL:** $\frac{\exp[a(\theta - b)]}{1 + \exp[a(\theta - b)]}$
:::

</br>


::: {.fragment .fade-in fragment-index="2"}
 **2MPL:** $\frac{1}{1 +\exp[-(a_{1}\theta_{1} + a_{2}\theta_{2} + d)]}$
:::
</br>


::: {.fragment .fade-in fragment-index="3"}
 **GLL~la~** and **GLL~ua~** [@Zhang_etal_2022]
:::
</center>

:::
:::col
::: r-stack


::: {.fragment .fade-in-then-out fragment-index="3"}
</br>

![](images/Stukel_fig.png){width=90%}
:::

::: {.fragment .fade-in fragment-index="4" .absolute bottom=160 left=0}

- 4 data-generating models (2PL, 2MPL, GLL~la~, GLL~ua~)
- 2 sample sizes (*N* = 100, 250)
- 2 test lengths (*I* = 10, 20)
- 100 replications (*R*)
- 9 quantiles (*q* = .10,...,.90)
:::



:::
:::
:::
::: {.fragment .fade-in fragment-index="6" .absolute bottom=0 left=0}
We will compare the performance of model selection (MS), test averaging (TA), item averaging (IA), and kernel smoothing IRT (KS):

$$RMSE_{q} = \sqrt{\frac{1}{100}\sum_{r=1}^{100}\frac{1}{I}\sum_{i = 1}^{I}[P_r(y_{in} = 1|\theta_{q}) - \hat{P}_r(y_{in} = 1|\tilde{\theta}_{q})]^{2}}$$
:::

## Distribution of Test and Item Weights 



![](images/plot_weight_distribution.png){fig-align="center"}



## Performance of BMA+ Over Stacking Weights 

![](images/plot_BMA_VS_Stack.png){fig-align="center" width=150%}



## Comparison of Item Averaging at Theoretical and Empirical Quantiles (*N* = 100)


![](images/Emp_vs_Theo_100_IA.png){fig-align="center"}

## Comparison of Test Averaging at Theoretical and Empirical Quantiles (*N* = 100)

![](images/Emp_vs_Theo_100_MA.png){fig-align="center"}

## Comparison of Averaging Methods, Model selection (MS), Kernel Smoothing (KS) for *I* = 10

![](images/results_compare_10_it.png){fig-align="center"}

## Comparison of Averaging Methods, Model selection (MS), Kernel Smoothing (KS) for *I* = 20

![](images/results_compare_20_it.png){fig-align="center"}

## Summary of Results

::: {.fragment .fade-in fragment-index="1"}  
- Item weights may provide useful insight into individual item behavior. 
:::

</br>

::: {.fragment .fade-in fragment-index="2"}  
- Either item weights or test weights can provide a stable method of detecting and estimating asymmetry in small sample sizes. 
:::

</br>

::: {.fragment .fade-in fragment-index="3"}  
- Item level averaging, followed by test level averaging, consistently offered better IRF recovery. 
:::

</br>

::: {.fragment .fade-in fragment-index="4"}  
- BMA+ weights showed better performance for IA, while stacking weights and BMA + weights performed similarly in the case of TA. 
:::

</br>

## Acknowledgement & Contacts

::: {layout-ncol="2"}
:::col


<center>
**Acknowledgements**
</center>

- **My advisor:** Leah Feuerstahler ([leah\@fordham.edu](mailto:leah@fordham.edu))

</br>
</br>

- Fordham's [High Performance Computing team](https://www.fordham.edu/academics/research/office-of-research/initiatives-and-infrastructure/internal-funding-opportunities/high-performance-computing/), especially Andrew Angelopoulos ([aangelopoulos\@fordham.edu](mailto:aangelopoulos@fordham.edu))

![](images/HPC_image.png){width=90%}

:::

:::col

<center>
**Contact and slides link**
</center>

- my email: [fsetti\@fordham.edu](mailto:fsetti@fordham.edu)

</br>

<center>

Slides QR code:

![](images/qr_code_presentation.svg){}

</center>

:::
:::


## References

<div id="refs"></div>


# Appendix


## Model Priors 

<center>

Model estimation used the partial pooling approach in the estimation of model parameters (i.e., random intercepts/slopes)

</br>
</br>

::: {layout-ncol="2"}
:::col

<center>

$log(\bar{a}) = N(0, .5)$

$\sigma_{log(\bar{a})} = \mathrm{Exponential}(3)$

$\bar{b} = N(0, 1)$

$\sigma_{\bar{b}} = \mathrm{Lognormal}(.25, .5)$

$\bar{\theta} = 0$

$\sigma_\bar{\theta} = 1$

</center>
:::

:::col

![](images/model_priors.png){fig-align="center"}
:::
:::



## IRF Averaging Scheme

</br>

<center>


**1. Posterior distributions of parameters:** The MCMC sampler provides $N$ (6000 in this simulation) draws for each model parameter.

 

 &darr;
 
 **2. Posterior distributions of $P(Y = 1|\theta_q)$:** The predicted probability of a keyed response is calculated at a set of empirical or theoretical quantiles , $P(Y = 1|\theta_q)$, for all models across all MCMC draws. 


 &darr;
 
**3. Model weights:** A weight (stacking or BMA+) is calculated for each model.

  

 &darr;
 

 **4. Averaged distribution of $P(Y = 1|\theta_q)$:** sample from each model distribution estimated in step 2 **in proportion to model weight**.


</center>

## All BLOT items Predictions

![](images/empirical_example/Real Data Predictions All Items.png){fig-align="center"}

## Parameter Generating Distributions for Simulation

</br>

![](images/Tab_datagen.png){fig-align="center" width=60%}



## LOO diffrence/SE for *I* = 10

![](images/loo_diff_10_it.png){fig-align="center"}



## LOO diffrence/SE for *I* = 20


![](images/loo_diff_20_it.png){fig-align="center"}

## Comparison of Item Averaging at Theoretical and Empirical Quantiles (*N* = 250)


![](images/Emp_vs_Theo_250_IA.png){fig-align="center"}

## Comparison of Test Averaging at Theoretical and Empirical Quantiles (*N* = 250)

![](images/Emp_vs_Theo_250_MA.png){fig-align="center"}
