{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Bayesian Model Averaging of (a)symmetric IRT Models in Small Samples\"\n",
        "title-slide-attributes:\n",
        "  \n",
        "   data-background-image: images/\n",
        "   data-background-size: 83%\n",
        "   data-background-opacity: 100%\n",
        "\n",
        "author: \"Fabio Setti & Leah Feuerstahler\"\n",
        "format:\n",
        "   revealjs:\n",
        "      footer: \"IMPS 2024\"\n",
        "      chalkboard: true \n",
        "      theme: Theme/IMPS_2024_theme.scss\n",
        "      navigation-mode: linear\n",
        "      controls: false\n",
        "      slide-number: c \n",
        "      width: 1280\n",
        "      height: 720\n",
        "      transition-speed: slow\n",
        "      css: Style.css\n",
        "      \n",
        "  \n",
        "editor: source\n",
        "---"
      ],
      "id": "dbbd9181"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Item Response Theory\n",
        "\n",
        "**Item response theory (IRT)** is a branch of psychometrics that models how both **person** and **item** characteristics influence the probability of discrete responses to item.   \n",
        "\n",
        "- **Item parameter:** characteristics of the item itself ($i$).\n",
        "\n",
        "- **Person parameter:** Some latent trait ($\\theta$) varying across individuals ($j$).\n",
        "\n",
        "\n",
        "\n",
        "::: {layout-ncol=\"2\"}\n",
        "\n",
        ":::col\n",
        "\n",
        "</br>\n",
        "\n",
        "</br>\n",
        "For example, the 2 parameter logistic model (2PL): \n",
        "\n",
        "\n",
        "$$P(Y^j_{i} = 1| \\theta_j, a_i, b_i) =  \\frac{\\exp[a_i(\\theta_j - b_i)]}{1 + \\exp[a_i(\\theta_j - b_i)]}$$\n",
        "</br>\n",
        "\n",
        "<center>\n",
        "$a$ = discrimination\n",
        "\n",
        "$b$ = difficulty\n",
        "\n",
        "$\\theta$ = latent trait\n",
        "\n",
        "</center>\n",
        "\n",
        ":::\n",
        "\n",
        ":::col\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "\n",
        "![](images/two_pl.png)\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "## Adding parameters\n",
        "\n",
        "::: {.panel-tabset}\n",
        "\n",
        "## 1PL\n",
        "\n",
        "\n",
        "::: {layout-ncol=\"2\"}\n",
        "\n",
        ":::col\n",
        "\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "<center>\n",
        "\n",
        "$$P(Y = 1| \\theta, b) =  \\frac{\\exp[(\\theta - b)]}{1 + \\exp[(\\theta - b)]}$$\n",
        "\n",
        "\n",
        "$\\theta$ = latent trait\n",
        "\n",
        "$b$ = difficulty\n",
        "\n",
        "</center>\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        ":::col\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "\n",
        "![](images/one_pl.png)\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "## 2PL\n",
        "\n",
        "\n",
        "::: {layout-ncol=\"2\"}\n",
        "\n",
        ":::col\n",
        "\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "<center>\n",
        "\n",
        "$$P(Y = 1| \\theta, b, a) =  \\frac{\\exp[a(\\theta - b)]}{1 + \\exp[a(\\theta - b)]}$$\n",
        "\n",
        "$\\theta$ = latent trait\n",
        "\n",
        "$b$ = difficulty\n",
        "\n",
        "$a$ =  discrimination\n",
        "\n",
        "</center>\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        ":::col\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "\n",
        "![](images/two_pl.png)\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "## 3PL\n",
        "\n",
        "\n",
        "\n",
        "::: {layout-ncol=\"2\"}\n",
        "\n",
        ":::col\n",
        "\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "<center>\n",
        "\n",
        "$$P(Y = 1| \\theta, b, a, c) =  c + (1-c) \\frac{\\exp[a(\\theta - b)]}{1 + \\exp[a(\\theta - b)]}$$\n",
        "\n",
        "$\\theta$ = latent trait\n",
        "\n",
        "$b$ = difficulty\n",
        "\n",
        "$a$ =  discrimination\n",
        "\n",
        "$c$ = guessing\n",
        "\n",
        "</center>\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        ":::col\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "\n",
        "![](images/three_pl.png)\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 4PL\n",
        "\n",
        "::: {layout-ncol=\"2\"}\n",
        "\n",
        ":::col\n",
        "\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "\n",
        "<center>\n",
        "\n",
        "$$P(Y = 1| \\theta, b, a, c, d) =  c + (d-c) \\frac{\\exp[a(\\theta - b)]}{1 + \\exp[a(\\theta - b)]}$$\n",
        "\n",
        "$\\theta$ = latent trait\n",
        "\n",
        "$b$ = difficulty\n",
        "\n",
        "$a$ = discrimination\n",
        "\n",
        "$c$ = guessing\n",
        "\n",
        "$d$ = slipping\n",
        "\n",
        "</center>\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        ":::col\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "\n",
        "![](images/four_pl.png)\n",
        "\n",
        ":::\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Estimation and Sample Size \n",
        "\n",
        "The more item parameters are added to the model, the more flexible the item response functions (IRF).\n",
        "\n",
        "\n",
        "However, the additional parameters of the **3PL** and **4PL** tend to require large sample sizes ($N \\geq 1000$) to be stably estimated.\n",
        "\n",
        "</br>\n",
        "\n",
        "<center>  What about **1PL** and **2PL** models? </center>\n",
        "\n",
        "</br>\n",
        "\n",
        "\n",
        "::: {layout-ncol=\"2\"}\n",
        "\n",
        ":::col\n",
        "\n",
        "<center> **Maximum Likelihood** </center>\n",
        "\n",
        "</br>\n",
        "\n",
        "<i class=\"fa fa-solid fa-thumbs-up\" style=\"color: #9B3922;\"></i></i> 1PL models seem to be stably estimable with sample sizes as low as $N = 100$ (Finch & French, 2019).\n",
        "\n",
        "\n",
        "</br>\n",
        "\n",
        "<i class=\"fa fa-solid fa-thumbs-down\" style=\"color: #9B3922;\"></i> 2PL models seems to require a sample size of $N = 200$ or more (Drasgow, 1989; Liu & Yang, 2018).\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        ":::col\n",
        "<center> **Markov Chain Monte Carlo** </center>\n",
        "\n",
        "\n",
        "</br>\n",
        "\n",
        "<i class=\"fa fa-solid fa-thumbs-up\" style=\"color: #9B3922;\"></i></i> 1PL models showed good coverage when $N = 100$ and generally outperformed maximum likelihood (Finch & French, 2019).\n",
        "\n",
        "\n",
        "</br>\n",
        "\n",
        "<i class=\"fa fa-solid fa-thumbs-up\" style=\"color: #9B3922;\"></i> 2PL models   of $N = 100$ or more (Drasgow, 1989; Liu & Yang, 2018).\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<style>\n",
        ".vl {\n",
        "  border-left: 3px solid #9B3922;\n",
        "  height: 400px;\n",
        "  position: absolute;\n",
        "  left: 49%;\n",
        "  margin-left: 10px;\n",
        "  top: 43%;\n",
        "}\n",
        "</style>\n",
        "\n",
        "<div class=\"vl\"></div>\n",
        "\n",
        "\n",
        "\n",
        "## Asymmetric IRT Models\n",
        "\n",
        "::: {layout-ncol=\"2\"}\n",
        "\n",
        ":::col\n",
        "\n",
        "All the IRT models presented so far generate IRFs that are symmetric **symmetric**.\n",
        "\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "![](images/symmetric_irf.png)\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        ":::col\n",
        "\n",
        "Samejima (2000) was the first scholar to propose the use **asymmetric** IRT with her logistic positive exponent (LPE) model.\n",
        "\n",
        "\n",
        "</br>\n",
        "\n",
        "![](images/asymmetric_irf.png)\n",
        "\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "## Simple Asymmetric IRT Models\n",
        "\n",
        "The LPE has shown to have some identification issues in simulation studies (Lee & Bolt, 2018), especially in small sample sizes. Two recently proposed asymmetric IRT models (Shim et al., 2023) may help address this issue:\n",
        "\n",
        "::: {layout-ncol=\"2\"}\n",
        "\n",
        ":::col\n",
        "<center> \n",
        "\n",
        "**Complementary Log-Log (CLL)**\n",
        "\n",
        "$$ P(Y = 1| \\theta) = 1 - \\exp[-\\exp[a(\\theta - b)]]$$\n",
        "\n",
        "\n",
        "![](images/CLL_vs_3pl.png)\n",
        "\n",
        "</center>\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ":::col\n",
        "<center> \n",
        "\n",
        "**Negative Log-Log (NLL)**\n",
        "\n",
        "$$ P(Y = 1| \\theta) =  \\exp[-\\exp[-a(\\theta - b)]]$$\n",
        "![](images/CLL_vs_3plU.png)\n",
        "\n",
        "\n",
        "</center>\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "## What to do about Small Sample Sizes? \n",
        "\n",
        "Although the NLL and CLL may approximate more complex models, complex IRFs remain hard to approximate in small sample sizes ($N \\leq 250$) with a single model.\n",
        "\n",
        "</br>\n",
        "\n",
        "<center> Can we do better with **Bayesian model averaging (BMA)**? </center>\n",
        "\n",
        "</br>\n",
        "\n",
        "**Model averaging** takes into account model uncertainty by weighting model predictions according to their relative plausibility.\n",
        "\n",
        "The core of **BMA** is the *expected log pointwise predictive density*, $ELPD = \\sum log(p(y_i |y_{-i}, M_k))$, which is an approximation of leave-one-out cross validation ($LOO_{CV}$).\n",
        "\n",
        "</br>\n",
        "\n",
        "::: {layout-ncol=\"2\"}\n",
        "\n",
        ":::col\n",
        "\n",
        "**Two type of weights:**\n",
        "\n",
        "- BMA weights with Bayesian bootstrapping (BMA+)\n",
        "\n",
        "- Stacking weights\n",
        "\n",
        ":::\n",
        "\n",
        ":::col\n",
        "\n",
        "\n",
        "\n",
        "<center>\n",
        "**Idea:** How much better can BMA of simple symmetric and asymmetric models do compared to model selection (MS) in the context of IRT? \n",
        "</center>\n",
        "\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Averaging Predicted Probability: The Scale of $\\theta$ \n",
        "\n",
        "::: {layout-ncol=\"2\"}\n",
        "\n",
        "\n",
        ":::col\n",
        "<center> **Ideal Scenario** </center>\n",
        "\n",
        "Ultimately, one would like to get the best possible estimate of $P(Y = 1|\\theta)$ by averaging along the $\\theta$ continuum.\n",
        "![](images/avg_plot.png)\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        ":::col\n",
        "<center> **Reality** </center>\n",
        "\n",
        "However, the same person will get a different $\\theta$ depending on the model that is fit to the data. \n",
        "\n",
        "</br>\n",
        "</br>\n",
        "\n",
        "![](images/theta_plot.png)\n",
        "\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "## Empirical and Theoretical Quantiles of $\\theta$ \n",
        "\n",
        "Although the same person may be assigned a different $\\theta$ depending on the model, the **relative rank** of participants should be scale invariant.\n",
        "\n",
        "\n",
        "This means that instead of averaging IRFs at the same $\\theta$ values, it should be more sound to average the IRFs at the same $\\theta$ **quantile**. \n",
        "\n",
        "\n",
        "These quantiles will be estimated empirically for each of the models to be averaged. Thus, the averaged probability of a keyed response at each **empirical quantile** will be \n",
        "\n",
        "$$\\overline{IRF}_{q} = \\sum_{1}^{m} W_{m}P_{m}(\\theta_{mq})$$\n",
        "here $q$ represents a specific quantile, $m$ represents one of the candidate models, and $W$ represents the weight assigned to each model.\n",
        "\n",
        "</br>\n",
        "\n",
        "Form comparison, IRFs will also be averaged at **theoretical quantiles** of the standard normal distribution.\n",
        "\n",
        "# {}\n",
        "\n",
        "- Is it possible to leverage simple symmetric and asymmetric IRT models (1PL, 2PL, CLL, NLL) to recover complex IRFs in **small sample sizes**?\n",
        "\n",
        "- Which method among **BMA**, model selection (**MS**), and kernel smoothing IRT (**KS**) will produce better IRF recovery?\n",
        "\n",
        "- Will averaging at **empirical $\\theta$ quantiles** achieve better IRF recovery than averaging at **theoretical $\\theta$ quantiles**?\n",
        "\n",
        "- How will **stacking weights** and **BMA+** weights behave? \n",
        "\n",
        "## Simulation: Data Generation\n",
        "\n",
        "Most data generating conditions were designed to be \"realistic\", where the true data generating model is not included in the set of candidate models. There were **4 data generating conditions**:\n",
        "\n",
        "::: {layout-ncol=\"2\"}\n",
        "\n",
        ":::col\n",
        "\n",
        "\n",
        "- **2PL:** $\\frac{\\exp[a(\\theta - b)]}{1 + \\exp[a(\\theta - b)]}$\n",
        "\n",
        "This condition is meant to be a sort of \"control condition\", as this model will be included in the candidate models.\n",
        "\n",
        "- **2MPL:** $\\frac{1}{1 +\\exp[-(a_{1}\\theta_{1} + a_{2}\\theta_{2} + d)]}$\n",
        "\n",
        "This model is meant to simulate a condition in which the unidemensionality of $\\theta$ assumption is violated.\n",
        " \n",
        "- **GLL~ua~** and **GLL~la~**\n",
        "\n",
        "A relatively complex IRT model that allows for both symmetry and asymmetry (Zhang et al., 2023)\n",
        "\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        ":::col\n",
        "\n",
        "\n",
        "\n",
        "::: r-stack\n",
        "\n",
        "\n",
        "::: {.fragment .fade-in-then-out fragment-index=\"1\"}\n",
        "</br>\n",
        "\n",
        "![](images/Stukel_fig.png)\n",
        ":::\n",
        "\n",
        "::: {.fragment .fade-in-then-out fragment-index=\"1\"}\n",
        "</br>\n",
        "\n",
        "![](images/Stukel_fig.png)\n",
        ":::\n",
        "\n",
        "```{tex}\n",
        "\n",
        "\\begin{flushleft} \n",
        "\\begin{tabular}{cccccccc}\n",
        "\\toprule\n",
        "  \\multicolumn{2}{c}{\\textbf{2PL}} & \n",
        "    \\multicolumn{2}{c}{\\textbf{2MPL}} & \n",
        "    \\multicolumn{2}{c}{\\textbf{GLL\\textsubscript{la}}} & \n",
        "    \\multicolumn{2}{c}{\\textbf{GLL\\textsubscript{ua}}} \\\\\n",
        "  \\cmidrule(lr){1-2}\\cmidrule(lr){3-4}\\cmidrule(lr){5-6} \\cmidrule(lr){7-8}\n",
        "   Par & Dist & Par & Dist & Par & Dist & Par & Dist\\\\\n",
        "  \\cmidrule{1-8}\n",
        "   \\(\\theta\\) & \\(\\mathcal{N} (0, 1)\\) &  \\(\\theta_{1,2}\\) &  \\(\\mathcal{MVN} (0, 1), \\rho = .3\\) & \\(\\theta\\) & \\(\\mathcal{N} (0, 1)\\) & \\(\\theta\\) & \\(\\mathcal{N} (0, 1)\\)  \\\\\n",
        "   \\textit{a} & \\(\\mathcal{N} (1.5, 0.5)\\) & \\(a_1\\) & \\(\\mathcal{N} (1.5, 0.5)\\) & \\textit{a} & \\(\\mathcal{N} (1.5, 0.5)\\) & \\textit{a} & \\(\\mathcal{N} (1.5, 0.5)\\)  \\\\\n",
        "   \\textit{b} & \\(\\mathcal{N} (0, 1)\\) & \\(a_2\\) & \\(\\mathcal{N} (0.5, 0.25)\\) & \\textit{b} & \\(\\mathcal{N} (0, 1)\\) & \\textit{b} & \\(\\mathcal{N} (0, 1)\\) \\\\\n",
        "    &  & \\( d \\) &  \\(\\mathcal{N} (0, 1)\\) & \\(\\alpha_2\\) & \\(\\mathcal{N} (0, 0.5)\\)  & \\(\\alpha_1\\) & \\(\\mathcal{N} (0, 0.5)\\)  \\\\\n",
        "    \n",
        "   \\bottomrule\n",
        "\\end{tabular}\n",
        "\\begin{singlespace}\n",
        "  \\footnotesize{\\textit{Note.} The numbers in parentheses represent means and standard deviations respectively. 2PL = two-parameter logistic model; 2MPL = two-parameter multi-trait logistic model; GLL\\textsubscript{la} = generalized logistic link with upper asymptote parameter fixed at 0; GLL\\textsubscript{ua} = generalized logistic link with lower asymptote parameter fixed at 0.} \n",
        "\\end{singlespace}\n",
        "\\end{flushleft}\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        ":::\n",
        ":::\n"
      ],
      "id": "8daabdc6"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}